{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"deeplearning.ipynb","version":"0.3.2","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tsC-yPre2FOU","colab_type":"text"},"source":["feedforward >>一方通行\n",">\n","活性化関数が入ることでモデルに非線形性が与えられる>>ニューラルネットワーク(NN)\n",">\n","tensorflow playground>>ブラウザでDNNの働きを見れる"]},{"cell_type":"markdown","metadata":{"id":"dF5cC-ND2tD9","colab_type":"text"},"source":["最適化　重みの調整\n",">\n","凸最適化→線形モデル\n",">\n","非凸最適化、勾配ベース→DNN　複雑な曲線？のようなイメージ\n",">\n","非凸最適化、勾配フリー→とても難しい　離散的なグラフのイメージ\n",">\n"]},{"cell_type":"markdown","metadata":{"id":"rGJowIRD9LPo","colab_type":"text"},"source":["コスト関数（損失関数）→悪さを決めて、それが少なくなるようなパラメータを学習\n",">\n","NNの最適化は難しい\n","→正解の値とNNの出力値との差が埋まるように少しずつ調整する形をとる\n",">\n","DNNが何故うまくいくのか詳しくはわかっていない"]},{"cell_type":"markdown","metadata":{"id":"RRQFi3EM9-Be","colab_type":"text"},"source":["活性化関数に欲しい性質：学習が早い、出力の値域が扱いやすい、微分可能\n",">\n","確率として欲しい、0～1に正規化して出力したい　式が簡単→シグモイドユニットsigmoid,σ\n",">\n","NNの出力そのままほしい(回帰)→線形ユニットlinear\n",">\n","マルチヌーイ出力分布(カテゴリカル分布)　確率分布、画像分類\n","→Tahn　合計して１になる　確率として使いやすい正規化をする　-1～1の値をとる\n",">\n","学習が早い、性能がいい(勾配が消えないので学習が停滞しない)、微分が簡単→ReLu\n",">\n","ReLuより微妙に性能がいい、最近出てきた改良版→Swish\n",">\n","出力層→ソフトマックス"]},{"cell_type":"markdown","metadata":{"id":"LU0GgL2ECG44","colab_type":"text"},"source":["万能近似定理\n","非線形な隠れユニットを使っていれば3層のネットワークで問題は解ける　という話\n",">\n","中間層のニューロンの数をたくさん増やすなどしないとこれは実現しないのであくまで理論上の話\n",">これを無視して層を増やすことで現実的なニューロンの数で解決できるようになるのがディープラーニング"]},{"cell_type":"markdown","metadata":{"id":"QKbz3pWuDIqJ","colab_type":"text"},"source":["誤差逆伝播法　出力結果と正解との誤差を逆伝播(出力から入力の向き)で修正していく\n",">\n","GradientDescent 関数の最小値を探索する連続最適化問題の勾配法のアルゴリズム"]},{"cell_type":"markdown","metadata":{"id":"KSJutiSOJ4d6","colab_type":"text"},"source":["過学習の防止\n",">\n","L1L2正規化\n",">\n","Data Augmentation　学習データの水増し　画像の場合、反転や拡大縮小などで増やす　データに応じて使い分ける必要がある\n",">\n","Dropout ネットワークのうち何割かのノード(ランダム)を一時的に無効化する Baggingと同じで、無効化にするノードを毎回変えることでアンサンブル効果が得られる\n",">\n","半教師あり学習　ラベルのないデータを足して精度をあげる\n",">\n","Early stopping 過学習し始めたら学習をやめる validation lossが上がり始めたら止める\n",">\n"]},{"cell_type":"markdown","metadata":{"id":"lTfsmoCqOPA_","colab_type":"text"},"source":["バッジ学習→データすべてをつかう\n",">\n","オンライン学習→データの中からランダムに1個使う\n",">\n","ミニバッジ学習→それらの中間　DLはこれを使う\n",">\n","確率的勾配降下法(SGD) NNの最適化に使う"]},{"cell_type":"markdown","metadata":{"id":"k41Os50BAtGs","colab_type":"text"},"source":["CNN 畳み込みネットワーク\n",">\n","畳み込み、活性化関数（Acivation)、プーリング　を繰り返す\n",">\n","画像分類によく使われるdeep learning\n",">\n","畳み込みは入力にフィルタをかけて小さくする(Laplacian filter Sobel filter)\n","それで得られた画像を特徴マップという\n","フィルタ数＞出力チャンネル数→入力が小さくなる分これだけだとDLと相性が悪い　→paddingで対処\n","zero padding　画像の周りに0埋めの余白を作る。畳み込みで小さくなった分0を足す\n","pooling 大域的情報を見るために画像を縮小する　大抵1/2\n"]},{"cell_type":"markdown","metadata":{"id":"cYTD0lg-FpFy","colab_type":"text"},"source":[" 出力サイズ \n"," h’ = (h + 2p - k) / s + 1 \n"," w’ = (w + 2p - k) / s + 1 \n"," • h’, w’: 出力サイズ \n"," • p: パディングサイズ(padding size)\n"," • k: フィルタサイズ(ﬁlter size, kernel size)\n"," • s: ストライド(stride)(フィルタの移動間隔)"]},{"cell_type":"markdown","metadata":{"id":"Unh7GoFPGhFQ","colab_type":"text"},"source":["CNNはDLでフィルタを求める\n"]}]}